# Raymontology Alert Rules
# Prometheus AlertManager Configuration

groups:
  - name: raymontology_critical
    interval: 1m
    rules:
      # Service Down
      - alert: ServiceDown
        expr: up{job="raymontology"} == 0
        for: 3m
        labels:
          severity: critical
          team: devops
        annotations:
          summary: "Raymontology service is down"
          description: "Service {{ $labels.instance }} has been down for more than 3 minutes."

      # High Error Rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total[5m]))
          ) > 0.01
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "High 5xx error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes."

      # Database Connection Failure
      - alert: DatabaseConnectionFailure
        expr: sqlalchemy_pool_checked_out == 0 and sqlalchemy_pool_size > 0
        for: 1m
        labels:
          severity: critical
          team: database
        annotations:
          summary: "Cannot connect to database"
          description: "No active database connections for more than 1 minute."

  - name: raymontology_warning
    interval: 5m
    rules:
      # High CPU Usage
      - alert: HighCPUUsage
        expr: container_cpu_usage_seconds_total > 0.8
        for: 10m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is {{ $value | humanizePercentage }} for more than 10 minutes."

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.85
        for: 5m
        labels:
          severity: warning
          team: devops
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is {{ $value | humanizePercentage }} for more than 5 minutes."

      # Slow API Response
      - alert: SlowAPIResponse
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "API responses are slow"
          description: "P95 latency is {{ $value }}s for endpoint {{ $labels.endpoint }}."

      # Database Connection Pool Exhaustion
      - alert: DatabasePoolExhaustion
        expr: |
          (
            sqlalchemy_pool_checked_out
            /
            sqlalchemy_pool_size
          ) > 0.9
        for: 5m
        labels:
          severity: warning
          team: database
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "Connection pool is {{ $value | humanizePercentage }} full."

      # Celery Queue Backlog
      - alert: CeleryQueueBacklog
        expr: celery_queue_length > 100
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Celery queue has large backlog"
          description: "Queue {{ $labels.queue }} has {{ $value }} pending tasks."

  - name: raymontology_business
    interval: 15m
    rules:
      # High Risk Score Detected
      - alert: HighRiskScoreDetected
        expr: risk_score_distribution{le="0.9"} < risk_score_distribution{le="1.0"}
        for: 0m
        labels:
          severity: info
          team: analytics
        annotations:
          summary: "High risk score detected"
          description: "A company with risk score > 0.9 was detected."

      # DART Crawler High Failure Rate
      - alert: DARTCrawlerHighFailureRate
        expr: |
          (
            rate(dart_crawler_failure_total[1h])
            /
            rate(dart_crawler_total[1h])
          ) > 0.1
        for: 30m
        labels:
          severity: warning
          team: data
        annotations:
          summary: "DART crawler has high failure rate"
          description: "Crawler failure rate is {{ $value | humanizePercentage }} over the last hour."

      # Low Daily Active Users
      - alert: LowDailyActiveUsers
        expr: active_users < 10
        for: 1h
        labels:
          severity: info
          team: product
        annotations:
          summary: "Low daily active users"
          description: "Only {{ $value }} active users in the last hour."

  - name: raymontology_security
    interval: 5m
    rules:
      # Multiple Failed Login Attempts
      - alert: MultipleFailedLogins
        expr: rate(auth_failed_login_total[5m]) > 5
        for: 5m
        labels:
          severity: warning
          team: security
        annotations:
          summary: "Multiple failed login attempts detected"
          description: "{{ $value }} failed login attempts per second from {{ $labels.ip }}."

      # Unusual API Activity
      - alert: UnusualAPIActivity
        expr: rate(http_requests_total[5m]) > 100
        for: 10m
        labels:
          severity: info
          team: security
        annotations:
          summary: "Unusual API activity detected"
          description: "{{ $value }} requests per second from {{ $labels.ip }}."

# Slack Notification Config (Better UptimeÏúºÎ°ú ÎåÄÏ≤¥ Í∞ÄÎä•)
# receivers:
#   - name: slack-critical
#     slack_configs:
#       - api_url: 'YOUR_SLACK_WEBHOOK_URL'
#         channel: '#alerts-critical'
#         title: 'üö® Critical Alert'
#         text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}'
#
#   - name: slack-warning
#     slack_configs:
#       - api_url: 'YOUR_SLACK_WEBHOOK_URL'
#         channel: '#alerts-warning'
#         title: '‚ö†Ô∏è Warning Alert'
#         text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}'

# Route Configuration
# route:
#   group_by: ['alertname', 'severity']
#   group_wait: 10s
#   group_interval: 10s
#   repeat_interval: 12h
#   receiver: 'slack-warning'
#   routes:
#     - match:
#         severity: critical
#       receiver: slack-critical
#       continue: true
