"""
í‰ê°€ ëª¨ë“ˆ (Phase 6a + 6b)

ì •ëŸ‰ í‰ê°€ ë° ê±°ë˜ì •ì§€ ê¸°ì—… ê²€ì¦

Usage:
    cd backend
    DATABASE_URL="postgresql://..." python -m ml.training.evaluator --model-path path/to/model.pkl
"""

import argparse
import logging
import os
import sys
from typing import Dict, List, Tuple

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import numpy as np
import pandas as pd
import joblib

from sklearn.metrics import (
    roc_auc_score, precision_score, recall_score, 
    brier_score_loss, confusion_matrix, classification_report
)
from sqlalchemy import create_engine, text

from ml.config import (
    FEATURE_NAMES, SAVED_MODELS_DIR, DEFAULT_EVALUATION_CONFIG,
    EvaluationConfig, get_risk_level
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ModelEvaluator:
    """ëª¨ë¸ í‰ê°€ê¸° (Phase 6a + 6b)"""
    
    def __init__(self, conn, model_path: str = None, config: EvaluationConfig = None):
        self.conn = conn
        self.config = config or DEFAULT_EVALUATION_CONFIG
        self.model_data = None
        self.models = {}
        
        if model_path:
            self._load_model(model_path)
        else:
            self._load_latest_model()
    
    def _load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        self.model_data = joblib.load(path)
        self.models = self.model_data['models']
        logger.info(f"ëª¨ë¸ ë¡œë“œ: {path}")
    
    def _load_latest_model(self):
        """ìµœì‹  ëª¨ë¸ ë¡œë“œ"""
        model_files = sorted([
            f for f in os.listdir(SAVED_MODELS_DIR)
            if f.startswith('ensemble_') and f.endswith('.pkl')
        ], reverse=True)
        
        if model_files:
            self._load_model(os.path.join(SAVED_MODELS_DIR, model_files[0]))
        else:
            raise FileNotFoundError("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def evaluate_quantitative(self) -> Dict:
        """
        Phase 6a: ì •ëŸ‰ í‰ê°€
        
        Returns:
            í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("=" * 60)
        logger.info("Phase 6a: ì •ëŸ‰ í‰ê°€")
        logger.info("=" * 60)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        feature_cols = ', '.join(FEATURE_NAMES)
        df = pd.read_sql(text(f"""
            SELECT 
                mf.company_id, 
                {feature_cols},
                CASE 
                    WHEN sc.suspension_type = 'TYPE_A' THEN 1
                    WHEN (
                        SELECT COUNT(*) FROM financial_statements fs 
                        WHERE fs.company_id = mf.company_id 
                          AND fs.fiscal_year IN (2023, 2024) 
                          AND fs.net_income < 0
                    ) >= 2 THEN 1
                    ELSE 0
                END as label
            FROM ml_features mf
            LEFT JOIN suspension_classifications sc ON sc.company_id = mf.company_id
            WHERE mf.feature_date = '2024-12-31'
        """), self.conn)
        
        X = df[FEATURE_NAMES].fillna(0)
        y = df['label']
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        predictions = []
        weights = []
        metrics = self.model_data.get('metrics', {})
        
        for name, model in self.models.items():
            pred = model.predict_proba(X)[:, 1]
            predictions.append(pred)
            auc = metrics.get(f'{name}_auc', 0.5)
            weights.append(auc)
        
        weights = np.array(weights) / sum(weights)
        y_pred_proba = np.average(predictions, axis=0, weights=weights)
        y_pred_binary = (y_pred_proba >= 0.5).astype(int)
        
        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        results = {}
        
        results['auc_roc'] = roc_auc_score(y, y_pred_proba)
        results['brier_score'] = brier_score_loss(y, y_pred_proba)
        results['recall'] = recall_score(y, y_pred_binary, zero_division=0)
        
        # Precision@10%
        threshold_10pct = np.percentile(y_pred_proba, 90)
        y_pred_top10 = (y_pred_proba >= threshold_10pct).astype(int)
        results['precision_at_10'] = precision_score(y, y_pred_top10, zero_division=0)
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y, y_pred_binary)
        results['confusion_matrix'] = cm.tolist()
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        logger.info("\ní‰ê°€ ê²°ê³¼:")
        for name, value in results.items():
            if name != 'confusion_matrix':
                logger.info(f"  {name}: {value:.4f}")
        
        logger.info(f"\ní˜¼ë™ í–‰ë ¬:\n{cm}")
        
        # MVP ê¸°ì¤€ ì²´í¬
        if results['auc_roc'] >= self.config.mvp_auc_roc:
            logger.info(f"\nâœ… MVP AUC ë‹¬ì„±! ({results['auc_roc']:.4f} >= {self.config.mvp_auc_roc})")
        else:
            logger.warning(f"\nâš ï¸ MVP AUC ë¯¸ë‹¬ ({results['auc_roc']:.4f} < {self.config.mvp_auc_roc})")
        
        return results
    
    def evaluate_suspended_companies(self) -> Dict:
        """
        Phase 6b: ê±°ë˜ì •ì§€ ê¸°ì—… ê²€ì¦
        
        TYPE_A ê¸°ì—…ë“¤ì˜ ì˜ˆì¸¡ê°’ì´ ë†’ì€ì§€ í™•ì¸
        
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("\n" + "=" * 60)
        logger.info("Phase 6b: ê±°ë˜ì •ì§€ ê¸°ì—… ê²€ì¦")
        logger.info("=" * 60)
        
        # TYPE_A ê¸°ì—… í”¼ì²˜ ë¡œë“œ
        feature_cols = ', '.join(FEATURE_NAMES)
        df = pd.read_sql(text(f"""
            SELECT 
                mf.company_id,
                c.name,
                c.ticker,
                {feature_cols}
            FROM ml_features mf
            JOIN suspension_classifications sc ON sc.company_id = mf.company_id
            JOIN companies c ON c.id = mf.company_id
            WHERE sc.suspension_type = 'TYPE_A'
              AND mf.feature_date = '2024-12-31'
        """), self.conn)
        
        if len(df) == 0:
            logger.warning("TYPE_A ê¸°ì—… í”¼ì²˜ ë°ì´í„° ì—†ìŒ")
            return {}
        
        X = df[FEATURE_NAMES].fillna(0)
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        predictions = []
        weights = []
        metrics = self.model_data.get('metrics', {})
        
        for name, model in self.models.items():
            pred = model.predict_proba(X)[:, 1]
            predictions.append(pred)
            auc = metrics.get(f'{name}_auc', 0.5)
            weights.append(auc)
        
        weights = np.array(weights) / sum(weights)
        y_pred_proba = np.average(predictions, axis=0, weights=weights)
        
        # ë¶„ì„
        df['pred_prob'] = y_pred_proba
        df['risk_level'] = [get_risk_level(p) for p in y_pred_proba]
        
        total = len(df)
        
        # ì„ê³„ê°’ë³„ íƒì§€ìœ¨
        detection_at_50 = (y_pred_proba >= 0.5).sum() / total
        detection_at_30 = (y_pred_proba >= 0.3).sum() / total
        detection_at_70 = (y_pred_proba >= 0.7).sum() / total
        
        results = {
            'total_type_a': total,
            'detection_at_50pct': detection_at_50,
            'detection_at_30pct': detection_at_30,
            'detection_at_70pct': detection_at_70,
            'mean_prob': float(y_pred_proba.mean()),
            'median_prob': float(np.median(y_pred_proba)),
            'min_prob': float(y_pred_proba.min()),
            'max_prob': float(y_pred_proba.max()),
        }
        
        # ìœ„í—˜ ë“±ê¸‰ ë¶„í¬
        level_dist = df['risk_level'].value_counts().to_dict()
        results['level_distribution'] = level_dist
        
        logger.info(f"\nTYPE_A ê¸°ì—… ìˆ˜: {total}ê°œ")
        logger.info(f"\nì˜ˆì¸¡ê°’ í†µê³„:")
        logger.info(f"  í‰ê· : {results['mean_prob']:.4f}")
        logger.info(f"  ì¤‘ì•™ê°’: {results['median_prob']:.4f}")
        logger.info(f"  ìµœì†Œ/ìµœëŒ€: {results['min_prob']:.4f} / {results['max_prob']:.4f}")
        
        logger.info(f"\nì„ê³„ê°’ë³„ íƒì§€ìœ¨:")
        logger.info(f"  >= 0.30: {detection_at_30*100:.1f}%")
        logger.info(f"  >= 0.50: {detection_at_50*100:.1f}%")
        logger.info(f"  >= 0.70: {detection_at_70*100:.1f}%")
        
        logger.info(f"\nìœ„í—˜ ë“±ê¸‰ ë¶„í¬:")
        for level, count in level_dist.items():
            pct = count / total * 100
            logger.info(f"  {level}: {count}ê°œ ({pct:.1f}%)")
        
        # í•©ê²© ê¸°ì¤€ ì²´í¬
        if detection_at_50 >= self.config.min_detection_rate_at_50pct:
            logger.info(f"\nâœ… 50% ì„ê³„ê°’ íƒì§€ìœ¨ í•©ê²©! ({detection_at_50*100:.1f}% >= {self.config.min_detection_rate_at_50pct*100}%)")
        else:
            logger.warning(f"\nâš ï¸ 50% ì„ê³„ê°’ íƒì§€ìœ¨ ë¯¸ë‹¬ ({detection_at_50*100:.1f}% < {self.config.min_detection_rate_at_50pct*100}%)")
        
        # ë¯¸íƒì§€ ê¸°ì—… TOP 10
        false_negatives = df[df['pred_prob'] < 0.3].sort_values('pred_prob').head(10)
        if len(false_negatives) > 0:
            logger.info(f"\nâš ï¸ ë¯¸íƒì§€ ê¸°ì—… (ì˜ˆì¸¡ê°’ < 0.30):")
            for _, row in false_negatives.iterrows():
                logger.info(f"  {row['name']} ({row['ticker']}): {row['pred_prob']:.4f}")
        
        return results


def run_evaluation(model_path: str = None):
    """í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    url = os.environ.get('DATABASE_URL', '')
    if url.startswith('postgres://'):
        url = url.replace('postgres://', 'postgresql://', 1)
    if '+asyncpg' in url:
        url = url.replace('+asyncpg', '')

    engine = create_engine(url)
    
    with engine.connect() as conn:
        evaluator = ModelEvaluator(conn, model_path=model_path)
        
        # Phase 6a: ì •ëŸ‰ í‰ê°€
        quant_results = evaluator.evaluate_quantitative()
        
        # Phase 6b: ê±°ë˜ì •ì§€ ê¸°ì—… ê²€ì¦
        suspended_results = evaluator.evaluate_suspended_companies()
        
        # ì¢…í•© ë³´ê³ ì„œ
        logger.info("\n" + "=" * 60)
        logger.info("ì¢…í•© í‰ê°€ ë³´ê³ ì„œ")
        logger.info("=" * 60)
        
        all_passed = True
        
        # MVP ê¸°ì¤€ ì²´í¬
        if quant_results.get('auc_roc', 0) >= DEFAULT_EVALUATION_CONFIG.mvp_auc_roc:
            logger.info("âœ… AUC-ROC: í•©ê²©")
        else:
            logger.info("âŒ AUC-ROC: ë¶ˆí•©ê²©")
            all_passed = False
        
        if suspended_results.get('detection_at_50pct', 0) >= DEFAULT_EVALUATION_CONFIG.min_detection_rate_at_50pct:
            logger.info("âœ… TYPE_A íƒì§€ìœ¨ (50%): í•©ê²©")
        else:
            logger.info("âŒ TYPE_A íƒì§€ìœ¨ (50%): ë¶ˆí•©ê²©")
            all_passed = False
        
        if all_passed:
            logger.info("\nğŸ‰ ëª¨ë“  í‰ê°€ ê¸°ì¤€ í†µê³¼! í”„ë¡œë•ì…˜ ë°°í¬ ê°€ëŠ¥")
        else:
            logger.info("\nâš ï¸ ì¼ë¶€ í‰ê°€ ê¸°ì¤€ ë¯¸ë‹¬. ëª¨ë¸ ê°œì„  í•„ìš”")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ëª¨ë¸ í‰ê°€ (Phase 6)')
    parser.add_argument('--model-path', type=str, help='ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    args = parser.parse_args()

    run_evaluation(model_path=args.model_path)
