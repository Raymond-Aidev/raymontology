#!/usr/bin/env python3
"""
RaymondsIndex v3.0 Option C: ë¡œì»¬ ê°€ì¤‘ì¹˜ ìµœì í™”

âš ï¸ ì£¼ì˜: ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í”„ë¡œë•ì…˜ DB/ë°±ì—”ë“œì— ì–´ë–¤ ë³€ê²½ë„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- DBì—ì„œ READ ONLYë¡œ ë°ì´í„° ì¶”ì¶œ
- ëª¨ë“  ê³„ì‚°ì€ ë¡œì»¬ ë©”ëª¨ë¦¬ì—ì„œ ìˆ˜í–‰
- ê²°ê³¼ëŠ” ë¡œì»¬ íŒŒì¼(CSV, JSON)ë¡œë§Œ ì €ì¥

ì‚¬ìš©ë²•:
    # 500ê°œ ìƒ˜í”Œë¡œ ê°€ì¤‘ì¹˜ ìµœì í™”
    python -m scripts.analysis.option_c_local_optimize --sample 500

    # ì „ì²´ ë°ì´í„°ë¡œ ë¶„ì„ (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)
    python -m scripts.analysis.option_c_local_optimize --full

    # ê²°ê³¼ë§Œ í™•ì¸
    python -m scripts.analysis.option_c_local_optimize --report
"""

import argparse
import asyncio
import csv
import json
import logging
import os
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import math
import random

import asyncpg

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë°ì´í„° í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class CompanySample:
    """ë¶„ì„ìš© ê¸°ì—… ìƒ˜í”Œ"""
    company_id: str
    company_name: str
    ticker: str
    is_failure: bool
    failure_reason: Optional[str]
    confidence: float
    cei_score: float
    rii_score: float
    cgi_score: float
    mai_score: float
    total_score: float


@dataclass
class OptimizationResult:
    """ìµœì í™” ê²°ê³¼"""
    method: str
    weights: Dict[str, float]
    f1_score: float
    precision: float
    recall: float
    auc_roc: float
    threshold: float
    cv_scores: List[float]
    cv_mean: float
    cv_std: float


@dataclass
class FinalReport:
    """ìµœì¢… ë¶„ì„ ë³´ê³ ì„œ"""
    timestamp: str
    sample_size: int
    failure_count: int
    normal_count: int

    # ê° ë°©ë²•ë³„ ê²°ê³¼
    lda_result: Optional[OptimizationResult]
    logistic_result: Optional[OptimizationResult]
    rf_result: Optional[OptimizationResult]

    # ì•™ìƒë¸” ê²°ê³¼
    ensemble_weights: Dict[str, float]
    ensemble_f1: float
    ensemble_auc: float

    # í˜„ì¬ ê°€ì¤‘ì¹˜ì™€ ë¹„êµ
    current_weights: Dict[str, float]
    improvement: float

    # ê¶Œì¥ì‚¬í•­
    recommendation: str


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¡œì»¬ ì „ìš© ë¶„ì„ê¸°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class OptionCLocalOptimizer:
    """
    Option C ë¡œì»¬ ê°€ì¤‘ì¹˜ ìµœì í™”ê¸°

    âš ï¸ í”„ë¡œë•ì…˜ ì˜í–¥ ì—†ìŒ:
    - DBëŠ” READ ONLY ì ‘ê·¼
    - ëª¨ë“  ê³„ì‚°ì€ ë©”ëª¨ë¦¬ì—ì„œ ìˆ˜í–‰
    - ê²°ê³¼ëŠ” ë¡œì»¬ íŒŒì¼ë¡œë§Œ ì €ì¥
    """

    CURRENT_WEIGHTS = {
        'CEI': 0.20,
        'RII': 0.35,
        'CGI': 0.25,
        'MAI': 0.20,
    }

    # ê³ ì‹ ë¢°ë„ ì‹¤íŒ¨ ì‚¬ìœ  (Tier 1-2)
    # Tier 1: ëª…í™•í•œ ì‹¤íŒ¨ ì§€í‘œ (ì¬ë¬´ì  ìœ„ê¸°ì˜ ëª…í™•í•œ ì¦ê±°)
    # Tier 2: ì ì¬ì  ì‹¤íŒ¨ ì§€í‘œ (ì™¸ë¶€ ê·œì œ/ê°ì‚¬ ê¸°ë°˜)
    TIER1_FAILURES = [
        'EMBEZZLEMENT',      # íš¡ë ¹/ë°°ì„ (ê°€ì¥ ëª…í™•í•œ ì‹¤íŒ¨)
        'CAPITAL_EROSION',   # ìë³¸ì ì‹ (ì¬ë¬´ì  ì‹¤íŒ¨)
        'CONTINUOUS_LOSS',   # 3ë…„ ì—°ì† ì ì (ì§€ì†ì  ì¬ë¬´ ì•…í™”)
    ]

    TIER2_FAILURES = [
        'AUDIT_QUALIFIED',   # ê°ì‚¬ì˜ê²¬ í•œì •
        'MANAGED_STOCK',     # ê´€ë¦¬ì¢…ëª©
    ]

    HIGH_CONFIDENCE_FAILURES = TIER1_FAILURES + TIER2_FAILURES

    def __init__(self, database_url: Optional[str] = None, tier1_only: bool = False):
        self.database_url = database_url or os.getenv('DATABASE_URL')
        if not self.database_url:
            raise ValueError("DATABASE_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        if self.database_url.startswith('postgresql+asyncpg://'):
            self.database_url = self.database_url.replace('postgresql+asyncpg://', 'postgresql://')

        self.tier1_only = tier1_only
        self.failure_types = self.TIER1_FAILURES if tier1_only else self.HIGH_CONFIDENCE_FAILURES

        self.samples: List[CompanySample] = []
        self.results_dir = os.path.join(
            os.path.dirname(__file__),
            '..', '..', 'data', 'option_c_results'
        )
        os.makedirs(self.results_dir, exist_ok=True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 1: ë°ì´í„° ì¶”ì¶œ (READ ONLY)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def extract_samples(self, sample_size: int = 500) -> List[CompanySample]:
        """
        DBì—ì„œ ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ (READ ONLY)

        âš ï¸ SELECT ì¿¼ë¦¬ë§Œ ì‚¬ìš©, INSERT/UPDATE/DELETE ì—†ìŒ
        """
        logger.info(f"ìƒ˜í”Œ ì¶”ì¶œ ì‹œì‘ (ëª©í‘œ: {sample_size}ê°œ)")

        conn = await asyncpg.connect(self.database_url)

        try:
            # 1. ì‹¤íŒ¨ ê¸°ì—… ì¶”ì¶œ (tier1_onlyì— ë”°ë¼ ë‹¤ë¥´ê²Œ)
            tier_label = "Tier 1 only" if self.tier1_only else "Tier 1-2"
            logger.info(f"ì‹¤íŒ¨ ê¸°ì—… í•„í„°: {tier_label} ({', '.join(self.failure_types)})")

            # ë™ì ìœ¼ë¡œ IN ì ˆ ìƒì„±
            placeholders = ', '.join(f"'{ft}'" for ft in self.failure_types)

            failure_rows = await conn.fetch(f"""
                SELECT DISTINCT
                    r3.company_id,
                    c.name as company_name,
                    c.ticker,
                    cl.failure_reason,
                    cl.confidence,
                    r3.cei_score,
                    r3.rii_score,
                    r3.cgi_score,
                    r3.mai_score,
                    r3.total_score
                FROM raymonds_index_v3 r3
                JOIN companies c ON c.id = r3.company_id
                JOIN company_labels cl ON cl.company_id = r3.company_id
                WHERE r3.fiscal_year = 2024
                  AND cl.label_type = 'FAILURE'
                  AND cl.confidence >= 0.85
                  AND cl.failure_reason IN ({placeholders})
                  AND r3.cei_score IS NOT NULL
                  AND r3.rii_score IS NOT NULL
                  AND r3.cgi_score IS NOT NULL
                  AND r3.mai_score IS NOT NULL
                ORDER BY cl.confidence DESC
                LIMIT $1
            """, int(sample_size * 0.25))  # ì‹¤íŒ¨ ê¸°ì—…ì€ 25%

            logger.info(f"ì‹¤íŒ¨ ê¸°ì—… ì¶”ì¶œ: {len(failure_rows)}ê°œ")

            # 2. ì •ìƒ ê¸°ì—… ì¶”ì¶œ (ì‹¤íŒ¨ ë ˆì´ë¸” ì—†ëŠ” ê¸°ì—…)
            normal_rows = await conn.fetch("""
                SELECT
                    r3.company_id,
                    c.name as company_name,
                    c.ticker,
                    r3.cei_score,
                    r3.rii_score,
                    r3.cgi_score,
                    r3.mai_score,
                    r3.total_score
                FROM raymonds_index_v3 r3
                JOIN companies c ON c.id = r3.company_id
                WHERE r3.fiscal_year = 2024
                  AND r3.cei_score IS NOT NULL
                  AND r3.rii_score IS NOT NULL
                  AND r3.cgi_score IS NOT NULL
                  AND r3.mai_score IS NOT NULL
                  AND NOT EXISTS (
                      SELECT 1 FROM company_labels cl
                      WHERE cl.company_id = r3.company_id
                        AND cl.label_type = 'FAILURE'
                  )
                ORDER BY RANDOM()
                LIMIT $1
            """, int(sample_size * 0.75))  # ì •ìƒ ê¸°ì—…ì€ 75%

            logger.info(f"ì •ìƒ ê¸°ì—… ì¶”ì¶œ: {len(normal_rows)}ê°œ")

            # ìƒ˜í”Œ ìƒì„±
            samples = []

            for row in failure_rows:
                samples.append(CompanySample(
                    company_id=str(row['company_id']),
                    company_name=row['company_name'],
                    ticker=row['ticker'] or '',
                    is_failure=True,
                    failure_reason=row['failure_reason'],
                    confidence=float(row['confidence']),
                    cei_score=float(row['cei_score']),
                    rii_score=float(row['rii_score']),
                    cgi_score=float(row['cgi_score']),
                    mai_score=float(row['mai_score']),
                    total_score=float(row['total_score']),
                ))

            for row in normal_rows:
                samples.append(CompanySample(
                    company_id=str(row['company_id']),
                    company_name=row['company_name'],
                    ticker=row['ticker'] or '',
                    is_failure=False,
                    failure_reason=None,
                    confidence=1.0,
                    cei_score=float(row['cei_score']),
                    rii_score=float(row['rii_score']),
                    cgi_score=float(row['cgi_score']),
                    mai_score=float(row['mai_score']),
                    total_score=float(row['total_score']),
                ))

            self.samples = samples
            logger.info(f"ì´ ìƒ˜í”Œ: {len(samples)}ê°œ (ì‹¤íŒ¨: {len(failure_rows)}, ì •ìƒ: {len(normal_rows)})")

            return samples

        finally:
            await conn.close()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 2: ê°€ì¤‘ì¹˜ ìµœì í™” (ìˆœìˆ˜ ë¡œì»¬ ê³„ì‚°)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def optimize_lda(self) -> OptimizationResult:
        """
        Method 1: Linear Discriminant Analysis

        Fisher's LDA ì§ì ‘ êµ¬í˜„ (sklearn ì˜ì¡´ì„± ì œê±°)
        """
        logger.info("LDA ìµœì í™” ì‹œì‘...")

        # ë°ì´í„° ì¤€ë¹„
        X = [[s.cei_score, s.rii_score, s.cgi_score, s.mai_score] for s in self.samples]
        y = [1 if s.is_failure else 0 for s in self.samples]

        # í´ë˜ìŠ¤ë³„ ë¶„ë¦¬
        class_0 = [X[i] for i in range(len(y)) if y[i] == 0]
        class_1 = [X[i] for i in range(len(y)) if y[i] == 1]

        if not class_0 or not class_1:
            logger.error("í´ë˜ìŠ¤ ë¶ˆê· í˜•: í•œ í´ë˜ìŠ¤ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return self._empty_result('LDA')

        # í´ë˜ìŠ¤ë³„ í‰ê· 
        mean_0 = [sum(x[i] for x in class_0) / len(class_0) for i in range(4)]
        mean_1 = [sum(x[i] for x in class_1) / len(class_1) for i in range(4)]

        # í´ë˜ìŠ¤ ë‚´ ë¶„ì‚°
        within_var = []
        for i in range(4):
            var_0 = sum((x[i] - mean_0[i])**2 for x in class_0) / len(class_0)
            var_1 = sum((x[i] - mean_1[i])**2 for x in class_1) / len(class_1)
            within_var.append((var_0 + var_1) / 2 + 1e-10)  # 0 ë°©ì§€

        # Fisher Ratio: (í´ë˜ìŠ¤ ê°„ ì°¨ì´)^2 / (í´ë˜ìŠ¤ ë‚´ ë¶„ì‚°)
        fisher_ratios = []
        for i in range(4):
            ratio = (mean_0[i] - mean_1[i])**2 / within_var[i]
            fisher_ratios.append(ratio)

        # ê°€ì¤‘ì¹˜ ë³€í™˜ (ì •ê·œí™”)
        total = sum(fisher_ratios)
        weights = {
            'CEI': fisher_ratios[0] / total,
            'RII': fisher_ratios[1] / total,
            'CGI': fisher_ratios[2] / total,
            'MAI': fisher_ratios[3] / total,
        }

        # ì„±ëŠ¥ í‰ê°€
        f1, precision, recall, threshold = self._evaluate_weights(weights, X, y)
        cv_scores = self._cross_validate(weights, X, y, k=5)

        return OptimizationResult(
            method='LDA',
            weights=weights,
            f1_score=f1,
            precision=precision,
            recall=recall,
            auc_roc=self._calculate_auc(weights, X, y),
            threshold=threshold,
            cv_scores=cv_scores,
            cv_mean=sum(cv_scores) / len(cv_scores) if cv_scores else 0,
            cv_std=self._std(cv_scores) if cv_scores else 0,
        )

    def optimize_logistic(self) -> OptimizationResult:
        """
        Method 2: Logistic Regression (ê²½ì‚¬í•˜ê°•ë²• ì§ì ‘ êµ¬í˜„)
        """
        logger.info("Logistic Regression ìµœì í™” ì‹œì‘...")

        X = [[s.cei_score, s.rii_score, s.cgi_score, s.mai_score] for s in self.samples]
        y = [1 if s.is_failure else 0 for s in self.samples]

        # ì •ê·œí™”
        X_norm, means, stds = self._normalize_features(X)

        # ì´ˆê¸° ê°€ì¤‘ì¹˜
        w = [0.25, 0.25, 0.25, 0.25]
        bias = 0.0
        lr = 0.01
        epochs = 1000
        lambda_reg = 0.1  # L2 ì •ê·œí™”

        # ê²½ì‚¬í•˜ê°•ë²•
        for _ in range(epochs):
            gradients = [0.0] * 4
            grad_bias = 0.0

            for i in range(len(X_norm)):
                # ì˜ˆì¸¡
                z = sum(w[j] * X_norm[i][j] for j in range(4)) + bias
                pred = 1 / (1 + math.exp(-max(-500, min(500, z))))

                # ì˜¤ì°¨
                error = pred - y[i]

                # ê·¸ë˜ë””ì–¸íŠ¸
                for j in range(4):
                    gradients[j] += error * X_norm[i][j]
                grad_bias += error

            # ì—…ë°ì´íŠ¸ (L2 ì •ê·œí™” í¬í•¨)
            for j in range(4):
                w[j] -= lr * (gradients[j] / len(X_norm) + lambda_reg * w[j])
            bias -= lr * (grad_bias / len(X_norm))

        # ê°€ì¤‘ì¹˜ ë³€í™˜ (ì ˆëŒ€ê°’ â†’ ë¹„ìœ¨)
        abs_w = [abs(wj) for wj in w]
        total = sum(abs_w)
        weights = {
            'CEI': abs_w[0] / total,
            'RII': abs_w[1] / total,
            'CGI': abs_w[2] / total,
            'MAI': abs_w[3] / total,
        }

        # ì„±ëŠ¥ í‰ê°€
        f1, precision, recall, threshold = self._evaluate_weights(weights, X, y)
        cv_scores = self._cross_validate(weights, X, y, k=5)

        return OptimizationResult(
            method='Logistic',
            weights=weights,
            f1_score=f1,
            precision=precision,
            recall=recall,
            auc_roc=self._calculate_auc(weights, X, y),
            threshold=threshold,
            cv_scores=cv_scores,
            cv_mean=sum(cv_scores) / len(cv_scores) if cv_scores else 0,
            cv_std=self._std(cv_scores) if cv_scores else 0,
        )

    def optimize_feature_importance(self) -> OptimizationResult:
        """
        Method 3: Feature Importance (ë‹¨ìˆœ ìƒê´€ê´€ê³„ ê¸°ë°˜)

        Random Forest ì—†ì´ ê°„ë‹¨í•œ ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚°
        """
        logger.info("Feature Importance ìµœì í™” ì‹œì‘...")

        X = [[s.cei_score, s.rii_score, s.cgi_score, s.mai_score] for s in self.samples]
        y = [1 if s.is_failure else 0 for s in self.samples]

        # ê° íŠ¹ì„±ê³¼ ë ˆì´ë¸”ì˜ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        correlations = []
        for j in range(4):
            feature_values = [X[i][j] for i in range(len(X))]
            corr = self._correlation(feature_values, y)
            correlations.append(abs(corr))  # ì ˆëŒ€ê°’ (ë°©í–¥ ë¬´ê´€)

        # ê°€ì¤‘ì¹˜ ë³€í™˜
        total = sum(correlations) + 1e-10
        weights = {
            'CEI': correlations[0] / total,
            'RII': correlations[1] / total,
            'CGI': correlations[2] / total,
            'MAI': correlations[3] / total,
        }

        # ì„±ëŠ¥ í‰ê°€
        f1, precision, recall, threshold = self._evaluate_weights(weights, X, y)
        cv_scores = self._cross_validate(weights, X, y, k=5)

        return OptimizationResult(
            method='FeatureImportance',
            weights=weights,
            f1_score=f1,
            precision=precision,
            recall=recall,
            auc_roc=self._calculate_auc(weights, X, y),
            threshold=threshold,
            cv_scores=cv_scores,
            cv_mean=sum(cv_scores) / len(cv_scores) if cv_scores else 0,
            cv_std=self._std(cv_scores) if cv_scores else 0,
        )

    def ensemble_optimize(self) -> Tuple[Dict[str, float], float, float]:
        """
        ì•™ìƒë¸”: 3ê°€ì§€ ë°©ë²•ì˜ ê°€ì¤‘ í‰ê· 

        ê°€ì¤‘ì¹˜: LDA(0.4) + Logistic(0.3) + FeatureImportance(0.3)
        """
        logger.info("ì•™ìƒë¸” ìµœì í™” ì‹œì‘...")

        lda = self.optimize_lda()
        logistic = self.optimize_logistic()
        fi = self.optimize_feature_importance()

        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜
        ensemble = {}
        for key in ['CEI', 'RII', 'CGI', 'MAI']:
            ensemble[key] = (
                0.4 * lda.weights[key] +
                0.3 * logistic.weights[key] +
                0.3 * fi.weights[key]
            )

        # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        X = [[s.cei_score, s.rii_score, s.cgi_score, s.mai_score] for s in self.samples]
        y = [1 if s.is_failure else 0 for s in self.samples]

        f1, _, _, _ = self._evaluate_weights(ensemble, X, y)
        auc = self._calculate_auc(ensemble, X, y)

        return ensemble, f1, auc, lda, logistic, fi

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 3: ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _evaluate_weights(
        self,
        weights: Dict[str, float],
        X: List[List[float]],
        y: List[int]
    ) -> Tuple[float, float, float, float]:
        """ê°€ì¤‘ì¹˜ë¡œ ì ìˆ˜ ê³„ì‚° í›„ F1, Precision, Recall, ìµœì  ì„ê³„ê°’ ë°˜í™˜"""

        # ìƒˆ ê°€ì¤‘ì¹˜ë¡œ ì ìˆ˜ ê³„ì‚°
        scores = []
        for x in X:
            score = (
                weights['CEI'] * x[0] +
                weights['RII'] * x[1] +
                weights['CGI'] * x[2] +
                weights['MAI'] * x[3]
            )
            scores.append(score)

        # ìµœì  ì„ê³„ê°’ íƒìƒ‰
        best_f1 = 0
        best_threshold = 0
        best_precision = 0
        best_recall = 0

        # ì‹¤íŒ¨ ê¸°ì—…ì€ ì ìˆ˜ê°€ ë‚®ë‹¤ê³  ê°€ì •
        for percentile in range(10, 90, 5):
            threshold = sorted(scores)[int(len(scores) * percentile / 100)]

            # ì˜ˆì¸¡: ì ìˆ˜ < ì„ê³„ê°’ â†’ ì‹¤íŒ¨
            predictions = [1 if s < threshold else 0 for s in scores]

            tp = sum(1 for i in range(len(y)) if predictions[i] == 1 and y[i] == 1)
            fp = sum(1 for i in range(len(y)) if predictions[i] == 1 and y[i] == 0)
            fn = sum(1 for i in range(len(y)) if predictions[i] == 0 and y[i] == 1)

            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
                best_precision = precision
                best_recall = recall

        return best_f1, best_precision, best_recall, best_threshold

    def _cross_validate(
        self,
        weights: Dict[str, float],
        X: List[List[float]],
        y: List[int],
        k: int = 5
    ) -> List[float]:
        """K-Fold êµì°¨ ê²€ì¦"""
        n = len(X)
        fold_size = n // k
        indices = list(range(n))
        random.shuffle(indices)

        cv_scores = []

        for i in range(k):
            # í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤
            test_start = i * fold_size
            test_end = (i + 1) * fold_size if i < k - 1 else n
            test_indices = set(indices[test_start:test_end])

            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            X_test = [X[j] for j in range(n) if j in test_indices]
            y_test = [y[j] for j in range(n) if j in test_indices]

            # í‰ê°€
            f1, _, _, _ = self._evaluate_weights(weights, X_test, y_test)
            cv_scores.append(f1)

        return cv_scores

    def _calculate_auc(
        self,
        weights: Dict[str, float],
        X: List[List[float]],
        y: List[int]
    ) -> float:
        """ê°„ë‹¨í•œ AUC ê³„ì‚° (ì •í™•í•œ ROCê°€ ì•„ë‹Œ ê·¼ì‚¬ì¹˜)"""
        scores = []
        for x in X:
            score = (
                weights['CEI'] * x[0] +
                weights['RII'] * x[1] +
                weights['CGI'] * x[2] +
                weights['MAI'] * x[3]
            )
            scores.append(score)

        # ì‹¤íŒ¨=1ì¸ ê²½ìš° ì ìˆ˜ê°€ ë‚®ë‹¤ê³  ê°€ì •
        # AUC â‰ˆ P(score_failure < score_normal)
        positives = [scores[i] for i in range(len(y)) if y[i] == 1]
        negatives = [scores[i] for i in range(len(y)) if y[i] == 0]

        if not positives or not negatives:
            return 0.5

        correct = 0
        total = 0
        for p in positives:
            for n in negatives:
                total += 1
                if p < n:  # ì‹¤íŒ¨ ê¸°ì—… ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ì •ë‹µ
                    correct += 1
                elif p == n:
                    correct += 0.5

        return correct / total if total > 0 else 0.5

    def _correlation(self, x: List[float], y: List[int]) -> float:
        """í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜"""
        n = len(x)
        if n == 0:
            return 0

        mean_x = sum(x) / n
        mean_y = sum(y) / n

        numerator = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n))
        denom_x = sum((xi - mean_x) ** 2 for xi in x) ** 0.5
        denom_y = sum((yi - mean_y) ** 2 for yi in y) ** 0.5

        if denom_x == 0 or denom_y == 0:
            return 0

        return numerator / (denom_x * denom_y)

    def _normalize_features(
        self, X: List[List[float]]
    ) -> Tuple[List[List[float]], List[float], List[float]]:
        """íŠ¹ì„± ì •ê·œí™” (Z-score)"""
        n = len(X)
        if n == 0:
            return X, [0]*4, [1]*4

        means = []
        stds = []

        for j in range(4):
            vals = [X[i][j] for i in range(n)]
            m = sum(vals) / n
            s = (sum((v - m)**2 for v in vals) / n) ** 0.5 + 1e-10
            means.append(m)
            stds.append(s)

        X_norm = []
        for i in range(n):
            X_norm.append([(X[i][j] - means[j]) / stds[j] for j in range(4)])

        return X_norm, means, stds

    def _std(self, values: List[float]) -> float:
        """í‘œì¤€í¸ì°¨"""
        if len(values) < 2:
            return 0
        mean = sum(values) / len(values)
        return (sum((v - mean)**2 for v in values) / len(values)) ** 0.5

    def _empty_result(self, method: str) -> OptimizationResult:
        """ë¹ˆ ê²°ê³¼"""
        return OptimizationResult(
            method=method,
            weights={'CEI': 0.25, 'RII': 0.25, 'CGI': 0.25, 'MAI': 0.25},
            f1_score=0,
            precision=0,
            recall=0,
            auc_roc=0.5,
            threshold=0,
            cv_scores=[],
            cv_mean=0,
            cv_std=0,
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 4: ê²°ê³¼ ì €ì¥ (ë¡œì»¬ íŒŒì¼ë§Œ)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def save_results(
        self,
        lda: OptimizationResult,
        logistic: OptimizationResult,
        fi: OptimizationResult,
        ensemble: Dict[str, float],
        ensemble_f1: float,
        ensemble_auc: float,
    ):
        """ê²°ê³¼ë¥¼ ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥ (DB ë³€ê²½ ì—†ìŒ)"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        tier_suffix = '_tier1' if self.tier1_only else '_tier12'

        # 1. JSON ê²°ê³¼ ì €ì¥
        result = {
            'timestamp': timestamp,
            'tier1_only': self.tier1_only,
            'failure_types': self.failure_types,
            'sample_size': len(self.samples),
            'failure_count': sum(1 for s in self.samples if s.is_failure),
            'normal_count': sum(1 for s in self.samples if not s.is_failure),
            'methods': {
                'LDA': asdict(lda),
                'Logistic': asdict(logistic),
                'FeatureImportance': asdict(fi),
            },
            'ensemble': {
                'weights': ensemble,
                'f1_score': ensemble_f1,
                'auc_roc': ensemble_auc,
            },
            'current_weights': self.CURRENT_WEIGHTS,
            'improvement': {
                'f1_delta': ensemble_f1 - 0.578,  # Phase 5 ê²°ê³¼ ëŒ€ë¹„
            },
        }

        json_path = os.path.join(self.results_dir, f'option_c_result{tier_suffix}_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        logger.info(f"JSON ê²°ê³¼ ì €ì¥: {json_path}")

        # 2. CSV ìš”ì•½ ì €ì¥
        csv_path = os.path.join(self.results_dir, f'option_c_summary{tier_suffix}_{timestamp}.csv')
        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Method', 'CEI', 'RII', 'CGI', 'MAI', 'F1', 'AUC', 'CV_Mean', 'CV_Std'])
            writer.writerow(['Current', '20%', '35%', '25%', '20%', '0.578', '-', '-', '-'])
            writer.writerow([
                'LDA',
                f"{lda.weights['CEI']:.1%}",
                f"{lda.weights['RII']:.1%}",
                f"{lda.weights['CGI']:.1%}",
                f"{lda.weights['MAI']:.1%}",
                f"{lda.f1_score:.3f}",
                f"{lda.auc_roc:.3f}",
                f"{lda.cv_mean:.3f}",
                f"{lda.cv_std:.3f}",
            ])
            writer.writerow([
                'Logistic',
                f"{logistic.weights['CEI']:.1%}",
                f"{logistic.weights['RII']:.1%}",
                f"{logistic.weights['CGI']:.1%}",
                f"{logistic.weights['MAI']:.1%}",
                f"{logistic.f1_score:.3f}",
                f"{logistic.auc_roc:.3f}",
                f"{logistic.cv_mean:.3f}",
                f"{logistic.cv_std:.3f}",
            ])
            writer.writerow([
                'FeatureImportance',
                f"{fi.weights['CEI']:.1%}",
                f"{fi.weights['RII']:.1%}",
                f"{fi.weights['CGI']:.1%}",
                f"{fi.weights['MAI']:.1%}",
                f"{fi.f1_score:.3f}",
                f"{fi.auc_roc:.3f}",
                f"{fi.cv_mean:.3f}",
                f"{fi.cv_std:.3f}",
            ])
            writer.writerow([
                'Ensemble',
                f"{ensemble['CEI']:.1%}",
                f"{ensemble['RII']:.1%}",
                f"{ensemble['CGI']:.1%}",
                f"{ensemble['MAI']:.1%}",
                f"{ensemble_f1:.3f}",
                f"{ensemble_auc:.3f}",
                '-',
                '-',
            ])
        logger.info(f"CSV ìš”ì•½ ì €ì¥: {csv_path}")

        # 3. ìƒ˜í”Œ ë°ì´í„° ì €ì¥
        samples_path = os.path.join(self.results_dir, f'samples{tier_suffix}_{timestamp}.csv')
        with open(samples_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'company_id', 'company_name', 'ticker',
                'is_failure', 'failure_reason', 'confidence',
                'CEI', 'RII', 'CGI', 'MAI', 'total_score'
            ])
            for s in self.samples:
                writer.writerow([
                    s.company_id, s.company_name, s.ticker,
                    s.is_failure, s.failure_reason or '', s.confidence,
                    f"{s.cei_score:.2f}", f"{s.rii_score:.2f}",
                    f"{s.cgi_score:.2f}", f"{s.mai_score:.2f}",
                    f"{s.total_score:.2f}",
                ])
        logger.info(f"ìƒ˜í”Œ ë°ì´í„° ì €ì¥: {samples_path}")

    def print_report(
        self,
        lda: OptimizationResult,
        logistic: OptimizationResult,
        fi: OptimizationResult,
        ensemble: Dict[str, float],
        ensemble_f1: float,
        ensemble_auc: float,
    ):
        """ê²°ê³¼ ì¶œë ¥"""
        tier_label = "Tier 1 only" if self.tier1_only else "Tier 1-2"
        print("\n" + "=" * 70)
        print(f"RaymondsIndex v3.0 Option C: ë¡œì»¬ ê°€ì¤‘ì¹˜ ìµœì í™” ê²°ê³¼ [{tier_label}]")
        print("=" * 70)
        print("âš ï¸  ì´ ë¶„ì„ì€ í”„ë¡œë•ì…˜ DBì— ì–´ë–¤ ë³€ê²½ë„ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print(f"ğŸ“Š ì‹¤íŒ¨ ê¸°ì—… í•„í„°: {', '.join(self.failure_types)}")
        print("=" * 70)

        print(f"\n## ìƒ˜í”Œ ì •ë³´")
        print(f"ì´ ìƒ˜í”Œ: {len(self.samples)}ê°œ")
        print(f"  - ì‹¤íŒ¨ ê¸°ì—…: {sum(1 for s in self.samples if s.is_failure)}ê°œ")
        print(f"  - ì •ìƒ ê¸°ì—…: {sum(1 for s in self.samples if not s.is_failure)}ê°œ")

        print(f"\n## ë°©ë²•ë³„ ìµœì  ê°€ì¤‘ì¹˜")
        print("-" * 70)
        print(f"{'Method':<20} {'CEI':>10} {'RII':>10} {'CGI':>10} {'MAI':>10} {'F1':>8} {'AUC':>8}")
        print("-" * 70)
        print(f"{'Current':<20} {'20.0%':>10} {'35.0%':>10} {'25.0%':>10} {'20.0%':>10} {'0.578':>8} {'-':>8}")
        print(f"{'LDA':<20} {lda.weights['CEI']:>9.1%} {lda.weights['RII']:>9.1%} {lda.weights['CGI']:>9.1%} {lda.weights['MAI']:>9.1%} {lda.f1_score:>8.3f} {lda.auc_roc:>8.3f}")
        print(f"{'Logistic':<20} {logistic.weights['CEI']:>9.1%} {logistic.weights['RII']:>9.1%} {logistic.weights['CGI']:>9.1%} {logistic.weights['MAI']:>9.1%} {logistic.f1_score:>8.3f} {logistic.auc_roc:>8.3f}")
        print(f"{'FeatureImportance':<20} {fi.weights['CEI']:>9.1%} {fi.weights['RII']:>9.1%} {fi.weights['CGI']:>9.1%} {fi.weights['MAI']:>9.1%} {fi.f1_score:>8.3f} {fi.auc_roc:>8.3f}")
        print("-" * 70)
        print(f"{'â˜… Ensemble':<20} {ensemble['CEI']:>9.1%} {ensemble['RII']:>9.1%} {ensemble['CGI']:>9.1%} {ensemble['MAI']:>9.1%} {ensemble_f1:>8.3f} {ensemble_auc:>8.3f}")

        print(f"\n## êµì°¨ ê²€ì¦ (5-Fold)")
        print("-" * 50)
        print(f"LDA:              Mean={lda.cv_mean:.3f}, Std={lda.cv_std:.3f}")
        print(f"Logistic:         Mean={logistic.cv_mean:.3f}, Std={logistic.cv_std:.3f}")
        print(f"FeatureImportance: Mean={fi.cv_mean:.3f}, Std={fi.cv_std:.3f}")

        print(f"\n## ê°€ì¤‘ì¹˜ ë³€í™” (Current â†’ Ensemble)")
        print("-" * 50)
        for key in ['CEI', 'RII', 'CGI', 'MAI']:
            current = self.CURRENT_WEIGHTS[key]
            new = ensemble[key]
            delta = (new - current) * 100
            direction = "â¬†ï¸" if delta > 0 else "â¬‡ï¸" if delta < 0 else "â†’"
            print(f"{key}: {current:.0%} â†’ {new:.1%} ({direction} {abs(delta):.1f}%p)")

        print(f"\n## ê¶Œì¥ ì‚¬í•­")
        print("-" * 50)
        if ensemble_f1 >= 0.7:
            print("âœ… F1 >= 0.7 ë‹¬ì„± - ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì ìš© ê¶Œì¥")
        else:
            print(f"âš ï¸ F1 = {ensemble_f1:.3f} < 0.7 - ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ í›„ ì¬ë¶„ì„ ê¶Œì¥")

        if ensemble_auc >= 0.75:
            print("âœ… AUC >= 0.75 ë‹¬ì„± - ì˜ˆì¸¡ë ¥ ì¶©ë¶„")
        else:
            print(f"âš ï¸ AUC = {ensemble_auc:.3f} < 0.75 - íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê²€í†  í•„ìš”")

        print("\n" + "=" * 70)
        print("ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜:", self.results_dir)
        print("=" * 70)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main():
    parser = argparse.ArgumentParser(
        description='RaymondsIndex v3.0 Option C: ë¡œì»¬ ê°€ì¤‘ì¹˜ ìµœì í™” (í”„ë¡œë•ì…˜ ì˜í–¥ ì—†ìŒ)'
    )
    parser.add_argument('--sample', type=int, default=500, help='ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸: 500)')
    parser.add_argument('--full', action='store_true', help='ì „ì²´ ë°ì´í„° ì‚¬ìš©')
    parser.add_argument('--report', action='store_true', help='ê¸°ì¡´ ê²°ê³¼ ì¶œë ¥ë§Œ')
    parser.add_argument('--tier1-only', action='store_true',
                        help='Tier 1 ì‹¤íŒ¨ë§Œ ì‚¬ìš© (EMBEZZLEMENT, CAPITAL_EROSION)')

    args = parser.parse_args()

    optimizer = OptionCLocalOptimizer(tier1_only=args.tier1_only)

    if args.report:
        # ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ ì¶œë ¥
        results_dir = optimizer.results_dir
        files = sorted([f for f in os.listdir(results_dir) if f.endswith('.json')], reverse=True)
        if files:
            latest = os.path.join(results_dir, files[0])
            with open(latest, 'r', encoding='utf-8') as f:
                result = json.load(f)
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            print("ì €ì¥ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. --sample ì˜µì…˜ìœ¼ë¡œ ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ìƒ˜í”Œ ì¶”ì¶œ
    sample_size = 10000 if args.full else args.sample
    await optimizer.extract_samples(sample_size)

    if len(optimizer.samples) < 50:
        logger.error(f"ìƒ˜í”Œì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {len(optimizer.samples)}ê°œ")
        return

    # ìµœì í™” ì‹¤í–‰
    ensemble, ensemble_f1, ensemble_auc, lda, logistic, fi = optimizer.ensemble_optimize()

    # ê²°ê³¼ ì €ì¥ (ë¡œì»¬ íŒŒì¼ë§Œ)
    optimizer.save_results(lda, logistic, fi, ensemble, ensemble_f1, ensemble_auc)

    # ë³´ê³ ì„œ ì¶œë ¥
    optimizer.print_report(lda, logistic, fi, ensemble, ensemble_f1, ensemble_auc)


if __name__ == '__main__':
    asyncio.run(main())
